{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea337813-fce5-4417-b59b-dc0d15fab55e",
   "metadata": {},
   "source": [
    "# Deep Learning - Report 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3d7fe-0654-415f-8fb3-cfb1007bda14",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "We are going to work on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) [Krizhevsky 2009].\n",
    "\n",
    "For the purpose of testing your skills, we are going to directly download an original dataset and manually adapt it to the PyTorch format. The following three cells download the data, create NumPy arrays of them, and show examples. The `load_cifar10` function converts the color images to gray-scale ones when `color=False`.\n",
    "\n",
    "[Krizhevsky 2009] [Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dc9eab2-e508-4bb2-b50b-13c83f57531b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "def download_cifar10():\n",
    "    filename = 'cifar-10.tar.gz'\n",
    "    if os.path.isfile(filename):\n",
    "        print(f'{filename} already exists. Skipping downloading.')\n",
    "        return\n",
    "\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "\n",
    "    with urllib.request.urlopen(url) as testfile, open('cifar-10.tar.gz', 'wb') as f:\n",
    "        f.write(testfile.read())\n",
    "\n",
    "\n",
    "def extract_cifar10(filename=\"cifar-10.tar.gz\"):\n",
    "    dirname = 'cifar-10-batches-py'\n",
    "    if Path(dirname).is_dir():\n",
    "        print(f'{dirname} already exists. Skipping extracting.')\n",
    "        return\n",
    "\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "def load_cifar10(train, dir='cifar-10-batches-py', color=False):\n",
    "    data_raw = []\n",
    "    if train:\n",
    "        for i in range(5):\n",
    "            with open(f'{dir}/data_batch_{i+1}', 'rb') as f:\n",
    "                data_raw.append(pickle.load(f, encoding='bytes'))\n",
    "        x = np.concatenate(\n",
    "            [d[b'data'] for d in data_raw],\n",
    "            axis=0)\n",
    "        y = np.concatenate(\n",
    "            [d[b'labels'] for d in data_raw],\n",
    "            axis=0)\n",
    "    else:\n",
    "        with open(f'{dir}/test_batch', 'rb') as f:\n",
    "            data_raw = pickle.load(f, encoding='bytes')\n",
    "        x = np.array(data_raw[b'data'])\n",
    "        y = np.array(data_raw[b'labels'])\n",
    "\n",
    "    x = np.reshape(x, newshape=(len(x), 3, 32, 32))\n",
    "    if not color:\n",
    "        x = x.mean(axis=1, keepdims=True)  # Convert Red-Green-Blue (RGB) images to gray-scale.\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a43ec81c-28c3-4349-9ebb-3b9252c6146e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10.tar.gz already exists. Skipping downloading.\n",
      "cifar-10-batches-py already exists. Skipping extracting.\n"
     ]
    }
   ],
   "source": [
    "download_cifar10()\n",
    "extract_cifar10()\n",
    "x_train_val_np, y_train_val_np = load_cifar10(train=True)\n",
    "x_test_np, y_test_np = load_cifar10(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "066184ca-ee34-49ce-b848-e5dd4e02c3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truck', 'truck', 'deer', 'automobile', 'automobile']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACZCAYAAABHTieHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzE0lEQVR4nO2dSbBeVdW/l92ngh0kJIEkQBLSQAghUSAQQLoCShEHljjRKqp05tSRE8dOLMsqqixnTLS0REUoURCKVgIB0hJCAgkhgTS0Yt9/g//k7mcv7jq5f857qc/nma2b9917n92dc/L+fnu95z//+c9/QkRERERE5B3mvbPdABERERER+b+JLxsiIiIiIjIKvmyIiIiIiMgo+LIhIiIiIiKj4MuGiIiIiIiMgi8bIiIiIiIyCr5siIiIiIjIKPiyISIiIiIio/D+oR+87bbbmnjXrl1NvH///rKMM888s4mXL1/exAsWLGjiD33oQ028ZcuWrsxnn322if/85z838Xve854m/sQnPjFtHRERl156aROvWrWqid96660m3rp1axP/61//6sr8+9//3sTbt29v4jfeeKOJ//a3v01b5ptvvtnVwWv/xz/+0cSnnHJKE7Mv2MasXn7mrrvu6r4zCf7973/PSr2zAefwn/70pyZ+5ZVXmnjOnDldGRy3E088sYn/53/+Z9o6mfuT/z5bvPe9k/v/ko997GNN/JGPfKSJsz55//vbLZbt5fpiGRzbbL/64Ac/2MR/+MMfmvjDH/5wE59wwglNzOuK6PeKo0ePNjHnIPnrX//a/Y3Xxr7hHOT+tGjRoibet29fWe/cuXOb+J///GcTc989/fTTuzLZ5x/4wAea+Pbbb+++Mwb33HNPE3MPzOYG/8b5x3X9vve9b9o2ZHOcZbCOaq/O8gqzHq6TmeQi5neqMrL7+PHWwWufSZksg/G111573GXOhG9+85tN/Je//KWJuZ4j+vXHcT311FObmM9FnPPZvpPVOxWugfnz5zcx98eIiCVLljTxVVdd1cR8tnr99deb+KMf/WhX5u7du5v47rvvbmL2DfcZ7tvZWuV9vlo3rIPXFdE/V3L+Pfroo913MvxlQ0RERERERsGXDRERERERGQVfNkREREREZBQGezaobaUunLquiF4PdsYZZzQx9bPUl1Gf9/vf/76rgxo1+j6ovWNMHXBEr+mjlvikk06atoxM90Z9Iz0Xr776ahOz76hLPPnkk7s6qE1kf/E62N/894iI1157rYmza5sNJqnVf7fxxz/+sYlffPHFJt65c2f3Hc6F6667rokrba30a7LSx0b0emB+h2Wy37nO6WOI6D0a3Ffp6aBH4+Mf/3hXJj093N9ZB6+TWuyIiCNHjjQxPS9Lly5tYvbF4sWLm5ga5oheF809jfOcZfD+kUHP3mzBa+OYRPT3GV4vy6Aem2OQ+S/oL2TM+ccyMh0+9eiZBn4qM/FCVMzEa8Lv8DOVR2tImbPlVzx06NC0/555J7g38Xni05/+dBNXz3OVVyyDPi62IYPPaxdffPG07aC3jnM+awf3fsK9f/Xq1U380ksvdd+pnpk5RowXLlzYlcnnymeeeeZtWjw9/71PbCIiIiIiMiq+bIiIiIiIyCj4siEiIiIiIqMw2LNBPSh1mdS4RfQeDer3qM+jD4SatpUrV3Z1XHLJJU1MzRn9FdREZmeTV+eEUxfH68i0xNTfrVixoomZt4TtpE44025Tf0efTXXmd6ZF5rjO5HzzMXi3tGMMqpwWL7zwQhM/9NBDTZzlOKA+nmPNtfduzatBJtku7kfcJ+bNm9d9h3seNfJZbpupMDcRfQsRfZ4f+iXoUyOZ/4kaefrYOD84fzIfCGFOC+7FlV+MuUCyz1D7z5jXnnnSuHeP4Q8YAucS20rdeETEyy+/3MTsY/p3qpwFmS+E/UFPWaZfn0qmw2c9Z511VhMvW7asiXldma/hnfY6ZPeg470vcf8aksdktuD84zhn7eRzCseAPhB6rg4cONDE2fpkn9E/ls3ZqWTPa5yTTz75ZBNzr6/qiKj9EuTss89uYs75LD8S/Zt8Tme72YYsd8dpp53WxENy6mX4y4aIiIiIiIyCLxsiIiIiIjIKvmyIiIiIiMgo+LIhIiIiIiKjMNggTqMJTWGZCYyJ6mg0YZISmruHJC2iYejgwYNNvHfv3iZmu3fs2NGVuXbt2ibeuHFjE9MIxcQpNOlE9P1DExOT1+zbt6+JaYDLkii+/vrrTUzzD02fLCMzPlYJwmaLd6th+Z2A84uGb5rmOK5MXBbRr0UaR2nWpVGMbfpvTKpIEzSNhTwQI6JPvMR1T1MvDeBc9/x+RMTy5cubmOZs7t3cV7MyaQZlwj3CvsgOKaBxnXOIpkwe7sF/z5JasV7uq2wn25DNa/YF+3NSPPzww03M/sjMnTyAgPs551eVZC67B9P0yzI536pEghH9vYkG3cOHDzcxDeTZHsixr5LlzSSp3/EypIwqMeCkYH9wrWXmba5hGsY5zjy4hP+e9Rf7g+uzOvAn26tY5mOPPdbENG/z8KLs4CHu01xLnNPsCz57XXPNNV0dPCyG65/XXj3XR/T7zEz3v/++JwYREREREZkIvmyIiIiIiMgo+LIhIiIiIiKjMNizQe0c9Y/UM0dErF+/vomZlIRaueeff76JmTSG3oiIPvHT0aNHm7hK6vfjH/+4K5O61csvv7yJqTmlFyXzE7z55ptN/NRTTzUxE4ZRh0+tXZaAj9pEJr2iRpA6frYh+xv7U/7/yDSonD8cJ64Tzq0sSRHnC+cfE79RX0+GtPv/GvSUcT1lPir6OJhckTpcemeYVJNem4g+6RzXLHW79HJl48a9lp/hHsh2ZkleOS+5dzNJFfv3yJEjTZx5TdhOtoNriZ/PxpD3qSEJC8eA2ukhSeo4FzhuVaIxjsEQTwu1+yyT45Z5TbguOI70RXJdZGNEPxTvj7y3Ze2aykz8FoRj+E4kChwL6vm5l2XPD7yPVD4E9jnLzBLhcX+rPAX0U2Q+VM4V7uO8DiZQ5jNjRO/JuPnmm5v4tttua+L777+/ieklZhwR8dxzzzXxPffc08SV13LDhg3d3zju1bPB2+EvGyIiIiIiMgq+bIiIiIiIyCj4siEiIiIiIqMw2LNRaeuG5Nl44YUXmpg6N+ouWUemN6Nej/pa6suYhyPT/VKvTB0cy6SOcNGiRV2Z1AAeOnSoibdt29bEPB+en8/0utTXUjtbnXeenZNNHXWmy5R3Fo4t82owBwvnZ+aroT6Za5HniPMM79nSqb+boY6ce1FEP5bU3XPvOHbs2LR1Zueg08NDDwf3OLY704RT10wdNMvgXp3lY6hyPBB69ngdmTeJ7ajymrC/mQcgK5P9Oyk4v7gXZ+OY7enTfafyvGR1ULtPDwfnPOdBdk/h2HKdZN6aqfDZI6K/r5944olNTO8lPaacS7x/RvT9Xflqhng23i1wvfIZhT7TiP6+wTXNca38FhdddFH3Nz6P7d69u4l5r2Od2b7NNX/ZZZc1Ma/j3nvvbeKnn366K5PXxvlXeZ/oU8ruBfwO/Zx8NmB+EPoSI/o98vrrr+8+MwR/2RARERERkVHwZUNEREREREbBlw0RERERERmFwZ4Nau+of9y/f3/3nR07drSVFVo56jCpH6X2M2sHtXTUs1NfumLFiq5MauF43vHSpUubeNWqVU2caXrpaWE7qOejrpXaWWrxInq9HjWWPBuafoxMA8gxysbgv4UhZ6Bzzs4k9wTHgePKMXjppZeamPkIIvr5RV/R448/3sTUL1944YVNXJ3XnVH1xUz0yrOZ24Ma7kyfXa0fami5R1I7nK0/6tvpW+O6596T+daoo2eZ1BvzOukRyr5DXn/99WnLpEY+K69aK1xbLGP+/PldmWxHtvdOAo4B13S2JitfzPHC+2tEP/84dzgmvI7svkO4Lqr9J8vHQHgt1PrT30NPAn2YEb3nJ1tbU+GeN+QeM1u+Do4j1062N/HetGbNmibmcw3L5LUy30VE/wzHNUzvDdcv52NE/7zGucIy+AxIH11Ev5fT17F3794m5vqm94Q55SIiLr300ibmff773/9+E/P5edOmTV2Z9IrceOON3WeG4C8bIiIiIiIyCr5siIiIiIjIKPiyISIiIiIiozDYs0EfAvNVMBdARK9Roz6MeTaoz+Pns7O1qQ+lXo86S+r7srOhmfOC7aLGlNrO8847rytz+fLlTczzjDdu3NjEzzzzTBPTw5HlNeFn2E7mMWEZc+bM6cok1fnmk6LySoxRR6aVpU6f7aC2eIing3+j5pReG2plszKZm4Nabs6dhx9+uInp4cj0ypMYk9mEY8mY4xLR9yt9B9QLVxrlbA5yTVL3zH2Uew89HRkcb2qYuQ9nXoHqXHn+O70AvI7sfHzueZXHjGOWtZtjxHvfpKi8DVVOhyFwzbLOzCvB/mEfchyH5O6gP4AxPRlsd9YX/Buvjc8rvK/TC5f5VOfOndvEXDd83mBfZXtm9fwxKbg+qzgi4vDhw03Mveb0009v4vXr1zcx13h27fTlcq5w/g3xMXEu0C9R1ZHNv8wbMhXuofRJc87zeS6i98TQn7d69eompvck86Nx36W3aSj+siEiIiIiIqPgy4aIiIiIiIyCLxsiIiIiIjIKgz0bPBN4586dTZx5Nqglpj6WPgbqzV577bUmPnbsWFdHdaYyvSY8yz3TqFGfx3ace+65Tbxy5comznTC1PBR88c8B+ecc04TU7v9yCOPdHVQw1edb85rz/wrbPe7Jc/GbHg0Mr0oz8I+3nPtM70yr41z+Jprrmnip556qomzM77pHaDemF4meoYefPDBJs7O2qY+9Hg9HEM05+9EHpOZcujQoSbmfOAZ+xF9Hg3q1TlfzjzzzGn/PdPMU9vLdpx11llNzHZneze159wr6NlgO+nHiOj3I44dddLcq7l/ZWPPds+bN6+Jt27d2sTMBzKkf7nmJwXXMMnWDzXy1H3zO5kPZrrvv93fpsJxmknuD85Z9sVM/HXHm6+C15Fp8Lku6O+hT5I+h6xv6PNiPCkWLFjQxHy+y9YO9x4+S1W+j8pbGNHvK5Wnhf03xCfDa6tyLGVtqNYB937WyZwZ9BRF9Hs5vSR8xuY+neXR4XM6fTZD8ZcNEREREREZBV82RERERERkFHzZEBERERGRUfBlQ0RERERERmGwQfy+++5rYpqFaGjO4Gcq4yLNQpkBiSabLNndVGhqygxHNPswMQ2N1JUhM4NGnC1btjQxTU9XXXVVEzM5S0Rv3mayGxp4aTzODHNMvFglpnk3c7yGQM5HmqYj+vnEwwNoJMvmcAW/w7lAs/d3v/vdrgyaWpmgiuuIyageffTRJuaBBRERGzZsaGKaTWkkpSE4O6yB841jQlPdmLAPjx492sSZKZrtZx/QoMwESjRUZrBP2I+sg3tNloyQf+M8p/Ga5kca4SP6pFXcr7g/cV5zHdC4nbWT+yjXI6/z1VdfLcuk6X9SVAb7IYnseL/jONE8yz7nGEb0c5rfYRuGGLWrZG5Vgk3uLVk9VTtYxkwOp2B/cw/hWh1yCMlsQbP7kHXA+cQ+pqG5MlFn/VONW7UGsjnNehhz/6u+H9E/J3KOc5y5NnkveeKJJ7o69uzZ08Q8MImHx7B/s6TNPKgjO0hoCP6yISIiIiIio+DLhoiIiIiIjIIvGyIiIiIiMgqDPRvUJ19wwQVNTL1tRK8Ho86N+mQmzRqS5ItazUojyTirg1pg6u8qX8gQ2F9MlEIdLK8r88hQQ0mNH5Mi0neT6QypZ2SSqNliiP+iSspXaXCZ3PG2227r6qDm9oorrmji66+/vom5JrL5V7WbOvMvfvGLTbxr166uzDvuuKOJqQfltdIvxb656667ujqo5eQ6yjwZU6GPJPsO2/3Vr3512jLfSaqkVllSP+rsua7Zzxx7+hpYXkS/l1x44YVNzLGlVjhL0lStFc5B7jWZN4ll0AtHbfCcOXOauPJjRPQacV4rE7By/WWeDX7m1FNP7T4zCTj3K09BRN923nM5H0877bTjrqNKMkffH71NmceR9zuWQe9NNhdI5dGonjeGeDSOtww+j/A6I/pkvbMF/WPcm7LnB/rF+AzINZ/5x6aS+Ve4xulFpW+B6yjrc857zkc+B1XJCSP6PZHXQp8knzO5brh/RvQ+Nyb+43rnOuIzePY3fmco/rIhIiIiIiKj4MuGiIiIiIiMgi8bIiIiIiIyCoM9G9TSUX9G7V1Er7mlrpKaNWq+q/OTs79RO0zPAcvMdJjUGbLdJ5xwwrRtyHJ3VDkGqHtlHbwuam8j+jG68cYbm3jHjh1NTA00c39kn8nOtp8NhowjNaaMOYepgXzooYeaePPmzV0d1HtyTq9bt66J58+f38ScFxH92LIO5pZYtGhRE3/961/vyqSW87HHHmtiznHq/NnObdu2dXVwTOgdoP+CGtQshwv1uEN8XGNBHe7atWubONMbs994zjnnC30f7LPsTHjOfc5zno/Psc78FZxz3NM4dpyzQ/wrHEv2Fe8fLDPzCnB/qs7cZ+6crH/fLb41jgE18tn59/TzcA6zP7je2H/0LWWfYRlcFxzH7J7Ca2G9Ve6E7B5MTxg9B4R1sL+z/Yo6e67N6nlliG+tavdYsF56H7IcDRzbuXPnNjHHmf3DtZbVUe1nfJ7jfSrLF/LSSy81MceJddJDmz0Pcx9mO7hvc/6x73h/jeg9ofv27Wti9i/LZBsj+n258mi9Hf6yISIiIiIio+DLhoiIiIiIjIIvGyIiIiIiMgqDPRs8i51axExLx89Qv1dp7Yac40z9GMugJpD6vMynUJ3ZzXOGeZ2Zrpxtrzwc1CoO8a/wjHjqdS+++OImfu6555p4+/btXZnsn3cix8hMqDS6GdRN3nfffU1MXwI/Tx069ZARvQaauUxY59KlS8syea419aLUy1PTm/lA6C/YunVrE1NnzetgO6mnj4jYsmVLE1PrzrU3RHvMeZ+dsz4puO65n2Vrg+fKs0/oE9q0adO0bcjOb6cGnuua5/RzrKkVzuA4cH5wz6SeO6L3odG/xL278utk3gnuC2xXlaujOuc/Yvb2wMqjkc0Nas95r+L1cw9kThH6wyJ6P1iVJ4j3pccff7wrk/Vy/3n22WebmGuT+2xWJvd/zlnOR+67mS6f7eQc5lrlmhhC5rGaBPQKModDlvuK65H7H/uQ48przfwCvCdwv+N3OB/pW4jox4XXtmbNmibmusrmBvcetov3fd7X2c7M38O1tX79+ibmfsD5SI9pRMS999477XeG4i8bIiIiIiIyCr5siIiIiIjIKPiyISIiIiIiozDYs1FpPalnjOh1lNSgUWvHMqjXyzSp1IGffPLJ08bUDPI8+YiIM844o4mp8eO1U5eYaRepN6bngtdR1ZGdqU4NYJXHYOXKlU1MvV9ExB133NHE1GpPCnoM6EugNjai101y/r344otNzPO4OX+pv8/KYDvvvPPOJq7y1UT0Y81x+9nPftbEnEuZrrrScjLvBtvA87cz/xS1nMxLwrXH9Z/5kLgOZuuM+YiIJUuWNDH7gB6EiHpsFi9e3MQ/+tGPpm1Dpi+mZp7zlP4aaoOzs9W5FrjfcF9lHZmnh2VQw8w8QNzPOPaZT5B/y/xLU+G6yOYg52nmR5kEvIdwvdGzF9FfT3YPnQr30dWrVzdx5mekh4V1Vv7ObBzpx6F3kHOJ/rqjR492ZXKsV6xY0cR79uxpYmriGWe5rnjv597NMaMXNusLlpHllJoECxcubGI+X2ReJu75nBvs09/85jdNfNpppzVx5ilgmexD+kDYzgMHDnRlcq3RA8S9iV4I+lki+rVV+XC5z3Dfzp7FODd4f+E9jO3MxpB9nvmhhuAvGyIiIiIiMgq+bIiIiIiIyCj4siEiIiIiIqPgy4aIiIiIiIzCYIM4zZA0t2TJyZj06qyzzmpiJk6hWZYGrMwcSsMbTc80e9MwSNNwRG/iZPIfmnxp+spMhplpfCrsi8qEmNVBUxNNnOw/GqgzA/TnP//5Jr799tunbddYPPDAA01Mc1pmfLzpppuamOapJ554ook5jjSG0qwW0RsZ2a5du3Y1MU3nWRIxmmtpRuO4st3894i+f66++uom5lygWZd9kRkw2U6aSRlzLWaHHtC8lx1EMSmqPS9bk1xzVTJP1kGDZQbN/NwDq/2MczKin0NsJ8u44YYbmjgzEdIgWRlwaV5kG7L1yPsUY14XTdWZoZxzLjuMYhLwfsp1nyUS432FMfuHhnAamPfu3VvWwXXOQw1YZ5YEl/sV5wbNypwbWdJXJq/k/OG1s07ek7P1znp5bXze4DMPzeDZZ7L+mgQ06XOcs72K99whhzxMR5VwOYPjxPmXJTXlnkgD+E9+8pMmXrZsWRNnRmuuXyYf5LWxnVzf2TzgtfIZmoeJ8PPZgTXcy7N7/xD8ZUNEREREREbBlw0RERERERkFXzZERERERGQUBns2Nm7c2MTUNzLhSETEqaee2sRMokOPAPVi1MNnekZqApm4hzpw6soz3Rt1brxWek+o08x0b1kStKlQ70h9KOvI9KLUG7Md1Brz85lPhMkGL7/88u4zk4BaYWqtV61a1X2HWmLOUeoXOVc437L5Rx0r5yPnPHXC1F1H9Fp1tmvBggVNTE15lvSJ84Vr87Of/WwTUx9KzSqvO6Ifk8w7MhWu/8wzxDGazaR+TA5FDW7m/+LYcR/guHA+cM5lCRsJ9yvqj6knzvx21MxzL6HWeoi/jnOfGmV6Tdh3nF+Z14R7Gr1G1b9nZXLfzNbsJOC6/9znPtfEzz//fPcdjhOvl9dGjyPvj/RwZHVw7Omv4BrI7ju8H1aeR47J4cOHu8/wb5x/vLY1a9Y0Mdci98SI/lrYf5xfVbLfrJ2zBe8rQ3y73Gs49uxT7hH0C2TPa3ymY51sN+9dTGCawQR6LIPris8eWbuYULoaZ84t1hnR9wXnE30i1X4YUSeDHoq/bIiIiIiIyCj4siEiIiIiIqPgy4aIiIiIiIzCYM/G+eef38Tr169vYuobI/ocAtTbUQdMnSY13JkWnbAOeh0YZzp8ejKos6Rej9eReTYqzSn/ndfBdmbniLNe9hd11Twr/5JLLunKpKaXmsBJQT08dcKZ1prfof6/0sdTv7hnz56uDup2OS5f+cpXmphz5+677+7KrLwB1GZTa5ydG865MH/+/Cbm+qa29pZbbmniTGfNeqkH5Zzmv2djyPk2RF87FvTGcM1melf6DtgH7Ed6jx599NEmzs6yp4+DGnmOPfs501rT+0btL6+V+uPMw8c5xXpZZ5UTI9M4swxeK9c411a2d/M+luXimASsd+3atU1Mr05E75/gnK009JyfWf4Ufod7M9vN+1A2pyuvErXn3BO5v0X0+QHoCeXefOaZZzYxcylkc3zr1q1N/OKLLzZxtWdU3s6I2Zt/Vb6K7JmEn6nyUp1zzjlNzP0wuwfTH1HlMuLcyp5p6Fmj34f7BP1UmWeNzwrV2iRZ/xLuiXx+4/7I+Zc9D/M5cab3YH/ZEBERERGRUfBlQ0RERERERsGXDRERERERGYXB4j/q4hhnemvq2qhJo5aYn6e+OdMzssyqjMrXENHreFlv5a/IzoKuzqiu+qpqQ1ZG1b/US2bnhi9ZsqSJZ0svWumC6XOIiPjhD3/YxPRHsH8OHDjQxNTbZnOF40KN/r333tvE9JHs3LmzK5OfoVadXhPmxKAmOqKe0/fdd9+0dVCnSf1zVibbRe0s52N2NvncuXOnLWOSVLlwMu8D5yk/wz7gPGfOlSyXx9VXXz1tHfQcsA+p643otb9c98zTQk3zwYMHuzIfeOCBJj7vvPOmLYMeK7aTczqiX6OcU9Rnc3wyePY/5/Wk4Ng/99xzTZzlwGDbeZ9mf9F3xTrnzJnT1cF1wDlMLwTvO8uXL+/KpJ6ddVT5ay6++OKuTLbjyJEjTcw5To08dfz0uUX0c5p7Me9TO3bsaGLmk4ro8zNkvqJJwDGhlj/LAcT5xvnEPr7ggguamB4hemIi+uct7tPVfSfzOBKuI14Hx4jXERGxadOmJt62bVsTsy84H4fk++Hez+dEfod9l93XuS9v2bKl+8wQ/GVDRERERERGwZcNEREREREZBV82RERERERkFAYL8Kn1ou4t075SD8YzfakBpCZtyBnU/A5jtot1ZmXyWqkNPumkk5qYer3sPOTqDGWef0ytNq8jazfHhO1gHWeccca0dUT0Y0j996Tgmfsk8yk89dRTTUwNOLWcHEdqP7Nz/Qn7lHOFc3rlypVdGfSO0LNBXSavndrkiNo/wXFmGdS6D9ENV+ews8xMg1/l6pgkHFu2n5ruiH6sOIdYBj0GnB9ZHy1evHjaMqpcEtl+xTlX5a9g3oPM/7V9+/YmphaYc5T70bp165o462/6BTgHGfM6sjXOfWDIefdjwDEYojXn9TJ3Fb0P2fyaCvfIrF2cX+zTXbt2NXGWE4NzmPsRc2Ft2LBh2joj+nGjH5G5YujpoA8py6vDucL7FvNIrFixoon37dvXlfnQQw81cba2JgH7h/2R5ZbgGmX/cH+nR4MeocxfRioPLffc7J7Ca628qpwrmbeQf2OZ7BveO+h1yjyC/Az7i96nY8eONXGWc4T7TLbvDsFfNkREREREZBR82RARERERkVHwZUNEREREREbBlw0RERERERmFwQbxX/ziF01MMwsNzRF5Aqqp0LxCkw2NOzS3RfQGLBpc2Ibdu3c3MQ01EbURlAZyGs0WLlzYlUnjE42JjFkHzUVDjIz8DhPT0ISXmc5ZBg3Pk4L10qSUjSMN4UxMx2urDg9g/0b0Ji8mxWEiH87XLIkOzZA0NnKdMDFXlnyQbae5kSYwlsHvZwbxykTH62B/Z2PIpH6zCfuE5trMXMd1TYMe90AaIq+44oomzuYgTbtsB+dxZZiM6E2/rINlcOwff/zxrkzOa/YnDeGVgTKDZfLwhLfeequJee2Z6ZfzckgiwDHg/sX1RLNnRG/KZxJR3qs433jPzRIaVvedKiFklsyT41IdJsE5znUW0ffXnj17mpiHCyxbtqyJ2ReHDh3q6qDBm4kX2Re8DvZNRMTll1/exDSMTwqa23nAw/PPP999h/ONSQ+5z3AucByzZxR+hvOPe8KQQ0b43MjnLcbcE3jIS9Yu7qm8r7MNvA4e4hFRm9+z543pvp/Vu2DBgmnLeDv8ZUNEREREREbBlw0RERERERkFXzZERERERGQUBns27rvvviZetGhRE2cegocffriJTz/99CamHpvJaqixpI8hok+W9+KLLzbxZZdd1sTnn39+E2faTuoqqf2n1nPr1q1NnCVGuummm6ZtB/V8TNRVtTGi74sqoRV9D5kOn5q/IcncxoD1Ukc4pD8yz89U6CFg/2R6RuoqqRHnv7OdWSKktWvXNjG1nNRNM9FiViZ11RxXrl/2NxMIZb4Qlsk5zTL5+cz3VXkDJgn9FRzbzM9UJVni2HGcqMPPfAtsR6Vfpy46W9PcexlTo8wyuWdmcJ5Sw8wy+fls76ZHhh4N6uyzeVyRJdOaBE8++WQTcx+g3yei92zs3bu3ia+88somvvXWW5v42muvbWL2b0Q/TtwnOU70nmSejcw7MxXOWe712TrhnPz2t7/dxNx/Nm7c2MRf+tKXmjhLRkiob2cdvI4sYR+fm84+++yy3jGgn+S6665r4scee6z7Dj0bTz/9dBMzaSmf34Yk0qUXjF46Uu1lEf19h9+pnouy5INsJ9cBEwmyXfS3MI7or53riO3idWQ+az6nZ76ZIfjLhoiIiIiIjIIvGyIiIiIiMgq+bIiIiIiIyCgMFkB/+ctfbmLqFTOtF/WiPNObmm5q5Kk3y7SK1OEzvuGGG6atY8iZ6dS1sV3UYb7wwgtdGdTX8YzqXbt2NTG1igcPHmxinr0d0ecHoc6Qmujq/P2IXgvLMZsUnF/V2doRfV6Rl19+uYl5Ljg1lexP9l9Ers2cCs9uZx+zfyN6jwZ15yyjOps8otdRU6vOMqo5n7Wbc5ZjQq0712KmQeUcni3PUEQ/LtRS068T0fcbdc/U6bIPOZaZb2jLli1v0+L/B/Xrq1atauJs7VTn2/PfK69JRJ9zgHkMqGGmP4frk3FEv0a5drIxmko2v7hWqvxRY8F6eU/I9P78zFVXXdXE3/rWt5r4e9/7XhMzvxb3xIh+7Kkb59yht4njnpXJec9xqjweERG33HJLE9NPwDp++tOfNvGKFSuamOsoot/jeO3sG669Ifsq951JUXm/Vq9e3X2Ha5T3XHo0+DyW9Qfhd3hP5pjw3pY999CbxDLpH+a1Z16w3/72t03MdvPewL2q2g+zvzGvFz1EnH/ZswPXWuYNG4K/bIiIiIiIyCj4siEiIiIiIqPgy4aIiIiIiIzCYM8G9Yz0GPDM7wxqD/kd6tyotcv8FfRLUKN65513NjF1cNT0RvRn31NjSq05z26nLi6i1xL/+te/buLdu3c3ceXZoNY7oteQ8jqoM6QukdcV0Y97dYb1WGS6yqkMyftA/0R1bv9rr73WxFmOh+occNZJDW/mA6Gfh74FXivXQNYXbCfHnuPMOqmdzbTt1FlTD8o6GGfnd7OM2fIMRfQ5Cqp8AhERy5cvb2JqaqnD5zixT7M6nnrqqSbmPKVXgtpgnqMe0ftAuJZuvvnmJuZ8ysqkj+Po0aNNzDnH/uXene2BXNMsg9p+9lW2nllPdm2T4Mwzz2xitjXzj3FPZ34swn+//fbbmzjz4nDss1wcU+GYZPedKncHnw24TjJvDnM8MIcIc1/94Ac/aGLmGqMnMGvngQMHmnjHjh1NzOugxj77THbPmATce7les7Yzjwb3P64t7jNDfJNcB8xtVO0rmQ+O84f3cdbJ57ch48gyqjwa/H72PMI9gDnf+B32d/Y8zPVM7/VQ/GVDRERERERGwZcNEREREREZBV82RERERERkFAZ7Nl599dUm/vnPf97E9C1E9Jo0aoup+65yFtx1113d36jV/NSnPjXtv7MOnvMc0V8rv0P/BM+O/uQnP9mV+Y1vfKOJf/e73zUxtXL0p1B3mJ2tv23btiamxpJ6PeofM63tkiVLmvimm27qPjMJKm115ueh54J6bs4/xtTHZz4F+gz4GWomqzHJqHKdsIwhuQIYV96IyjOT1VtdG9uQ1cF5z3iScKypX6dHKqL3ZLBP2AfVHM08BdS30wuxadOmJqbvKvOBcM5RL8y9nD61rC+YH4D5iNgX3JuXLl3axJnHh3s1NeG8LpaRaa0557L+mgRcH2w7xyiiHwfq2TlXuGdW/rGs3mqNcg1k7eZneG/id7hOspwj5Atf+EITb9y4sYl5n+czz+bNm7sy6QniHsH+5fhkeSX4nay/ZoMhOaO4L3AN79mzp4npjWA+t8wzWnmXGHNuZT5Azif2OfcA7jNZDgyWUd1jea1V/pCsTK539i/XavYcxfwyWa6dIfjLhoiIiIiIjIIvGyIiIiIiMgq+bIiIiIiIyCgM9mzwTGlq7zK9HjW4lUaemjVq0rLzkNmuz3zmM01M3Rs1ktu3b+/KfPbZZ5uYWmNeK70PzzzzTFcm/7Z48eImpsaUXgleO3XaEb2+lj4a6kep18s089Qesn8nBc+9phYx00hmf5sK845UWtjMC8Ez4itdJj1Ema+B4zDEL1G1k1DvXeXVoF40W++8Fsb8TpXrI6K/9uwzk4L5KebMmdPE2fyh/4tnp1f6Ys6X7Po5dkeOHGliapqZN2jNmjVdmdT2MicSc3dwzmXzg9fOvZvtolZ9586dTbx27dqujmrtcN7zuubNm9eVyb020zVPAvYH50qmPef10te3bt26Jt66dWsTV36xiH5+8T7EuVTl48ngdxhznLMxYn4UxvRNcr2z/3/1q191dbBezi+2k88OWY4ktjPL4/VuYIifh3mH6FHjfsn+zJ57Ku8I10CVGysro6qTazG7Z3Oec11Uz8fsC/ZVVgfL4HxjG7J281l1pvdgf9kQEREREZFR8GVDRERERERGwZcNEREREREZBV82RERERERkFGac1O/SSy9t4iuvvLL7Do0klXmF5pQhCa1ocKHRmt+pjI4RfaIjmgYr02ZmlLr33nubeOXKlU1MszLroNEqM8DR2E5DJtvFRDRMoBPRG5/uv//+Jv7a177WfWcMOP84rlmiLbad5qiqTzn/MiNjZuibCvucpunM8FaZbbNkZtN9f0g7adSr2p2VVx0IcbxG94g6mdckoYGUfcRDHSL6AwQ4x7juuZewT7kvZO3gWuAco7E9268455icku1g8sLzzjuvK5PJTyvD4/nnn9/ETz/9dBNn9wPOOe5xhNeVJQ3juHNMJwXXD9dkdk/gIRk8JOQ73/lOEzPRIpPUMdFd1q7KkMt9eSZJSFkn/z0z+PJaON+49ngoCdt94MCBro6qL7hu2M7MfMv5xmeeSVHtxTy0JaLvD5rbefgO9xEm9uRajOjHhX1ejUl2LzveQ1o4x7P5V93XGbNO1pEdLlM9N1Ztohk8K4NjNBR/2RARERERkVHwZUNEREREREbBlw0RERERERmFwZ4N6n6pnXviiSe679DrQE8AtXbU1/LfM6gxYwKc/fv3NzF1hVmCHOp4qVWkNnbRokVNTN9IRO8VqRJDUUdN3Vyms2Y7CTWq1OstXLiw+86QBDiToJoLmZa9SiJXkWkiSaUHrcrMfCDU7VaejSFeCJZBnSoTqnHc3wmdMMscklTyePt3TJgQlHMy6yOOJeNqjvHfM307/8Z2sQxeB/emiN7vxQShrJP7U+Z94PrjfsN9k1p17vVMshbRzym2k/cHzvtXXnmlK5PtyvaaSUCvDddwdk/hOPI71F+zf04++eQmztYo+5zrvPJwZGuaf6PfotK7Zzp8XuvDDz/cxFdffXUT0yPEa8/uwZVvjdc1JBkcvSGz5VvjvYrXmvkPq/sGn7/oHaT/LOtzfoZxlVw2g3OU7eK4Vp7I7DPsT65fzg22iftBRH+t7As+265ataqJhzwD6tkQEREREZF3Fb5siIiIiIjIKPiyISIiIiIiozDYs0F9GTVsDz74YPcdar2o6avOh6fGLTvHft26dU28bNmyJn755ZebmJpc5l6IiDjttNOamNrgNWvWNPG5557bxLfeemtXJjV+1DJWZ0Wzr7J289p5ZjrbQE306tWruzI5zpmmbxKccsopTcz5mOl+2WfUanK+sX8qve2QOgjbnWl0h3iVpiNrQ6WTptaz0mFn2tlK08t2VZ+P6PuH2u1Jwv2LevhMM89+Yhm8PvoB2Cf0wUX0GlrmVqjyMWQehMqzw/Pv6QPJ8ltwPTHHCDXI9ErQT5D5V3hta9eu7T4zFe5vjCP6Ps/m/iSovFvZOFLfzzVXeTJYZ9bnXJNsV+Uxy/Yr9jHnNOfjEB8I2/nLX/6yiXfu3NnEmzdvbuLs2snx5meovClZvbPpW5sK11q2f7PtHFfuG9wT6DNlnpiszCNHjjQxfXJ8dsrWTeWtqcYg82JyH2a9VV4v9m+2D1V+PfYF5xtzy2T1zhR/2RARERERkVHwZUNEREREREbBlw0RERERERmFwZ4N6vOoL7vhhhu671APVukZqQ2j3oweg4heG7xly5YmZl4Natb27NnTlUlfx/Lly5t448aNTUz9aHYONq+VfoFM4zeV6qzyiN7TwlwoPPuemtTnn3++K7M6w3pS8HzoIbkl2EfsD/b5EB8I4Zyt8kLw3zMd8BBN83Sfn0nejeqM75lQ9R/XSeZDqvaESUKNLfXGQ7T8bD/nKPud8yPzQmR5MqbCfua+nJ2FT+0uc2RQ68++4VqL6MeS7WIZ1G9zftBbF9F79AjbRY8GfTgR/RgMOad/DNh/M8n3wfsf4Zod4quq1iTn+JD9jfXSz1Pl6Mn2QF4bfSDU+vN+yntflStrSLsYZ88BM8npMAbVvSybB1yz3Gs49rzP83muWt8R/R7K/Y5rPms311b1rDBkbfJajzfvFOsYkp+G7WCZvJ8MWYtDni8y/GVDRERERERGwZcNEREREREZBV82RERERERkFAYLoHk+PLVh1PBG9Po8auOqM4ArrXFEr8GlRnDp0qXTlrl3796uTGok+Z2DBw82MfWkjCNqvWcVD/GFLFiwoIkPHTrUxDwbn5rVXbt2dWXybHvGk6I64zybG9RmUiNZnWtNbWKmya/KJKwj+3ylja3INL0sszo/P5tfFVU7Z6J1r3Suk4SesSovS0TvdWBODO6B/Dx9Ifz3iF6XSw/H4sWLm5h9mK1p7psc20pvTO11RN/2yu/Edlfn1EfUuZzYnyeddFITZ2NI31+Wi2MSvBNa/UpnX83prA2Vnr2aO0O8cdV+NcQvxe/Qw8hnB7abXoBsL6o8eZVfJfNsDMnFMQlm4hGqrp9riT4t3oeyca7maHVfysqs/BKE/56NI8e6uh9W+13mv+LfqrxN1XhE1F7XofjLhoiIiIiIjIIvGyIiIiIiMgq+bIiIiIiIyCj4siEiIiIiIqMw2CDORHc0LWWGUiZgYQI9GrJo2DrllFOmjSN6cxCNiTTh0BSdmSNPPfXUJj58+HAT7969u4mZ/GeIkZ0GcCbRqpKsZQnQaNLnd+bNm9fE559/fhPPnz+/K5N/ywyqk4AmLl5bloSOf6PJvko2NcT0xXGgIYt1cD5m5rXKpFkZMjM4n9ifVRIjtjNLBMe/sf84d1hHtm74mZkY198pKnPdkKSEVeJIXh/ncGZmpCmQ+wDnDxPXZUZXGqmr5J5D7gccf+6JrJPzaYgxuzJVsv9Yx7Fjx7oyOQazNQe5PoYcUMD5xrGuDOJkyCEPx7vuqzqHUN0fIuoDP9i/QxLpVlSHfXDtDklaykMQJkXVtpkkhKuS461bt66Jsz2Ah/zwMxznIQcSkOo7bPeQOc3vMObc4P1liEG8Spw6JGku+2+miZ39ZUNEREREREbBlw0RERERERkFXzZERERERGQUBns2qIGkPi/TtFEf9sgjjzTx0aNH28ZAL3bRRRc18caNG7s6mCRr8+bNTUyt+v79+5t43759XZn8DqEvhH6LN954o/sO9cmE2kX6V+gLybwmTILFJH9MmMP+zrT/la5wUlSa3CGJeTgfK13l8eqZs3awziEJ+6p2cgyG+ECo3aT+mDpM/vsQbXGVaIsxy6CXIGO25l9Erz3n9Qzpdyb85Hzg9dFDkO2z9DpQ010lbsq01hx/Jr+rElRleyj7gv656trpG+T+ln2mSiTIOrJ9mnvxkHk6GwzZA7mOK08ZPz/EY1Y9KwxJUlrdd1jnTPanai+uEldm/V2t5yq56hBPzDvhcZkJM0nqV3mGKh8c196GDRu6Oriv0CvM57PKm5P9rfJszMRHWc1xejSGPK9Vif8YD/FPcZ5Xz8dvh79siIiIiIjIKPiyISIiIiIio+DLhoiIiIiIjMJ7/jNEJCgiIiIiInKc+MuGiIiIiIiMgi8bIiIiIiIyCr5siIiIiIjIKPiyISIiIiIio+DLhoiIiIiIjIIvGyIiIiIiMgq+bIiIiIiIyCj4siEiIiIiIqPgy4aIiIiIiIzC/wJjWlJDMipJZQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "id2class = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "def plot_images(x, y, rows=1, cols=5, color=False):\n",
    "    figure = plt.figure(figsize=(2 * cols, 2 * rows))\n",
    "    ys = []\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            k = (i * cols) + j + 1\n",
    "            figure.add_subplot(rows, cols, k)\n",
    "            plt.axis(\"off\")\n",
    "            if color:\n",
    "                plt.imshow(np.transpose(x[k], [1, 2, 0]))\n",
    "            else:\n",
    "                plt.imshow(np.transpose(x[k], [1, 2, 0]), cmap=\"gray\")\n",
    "            ys.append(y[k])\n",
    "    print([id2class[id] for id in ys])\n",
    "    plt.show()\n",
    "\n",
    "plot_images(x_train_val_np, y_train_val_np, rows=1, cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7685e-d3ba-45e5-80a0-d48c2b61e2b7",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Randomly split the dataset `(x_train_val_np, y_train_val_np)` to a training set `(x_train_np, y_train_np)` and a validation set `(x_val_np, y_val_np)`. Here, take `40000` data points for the training set and put the rest in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3b24085-dd95-4b37-b1e0-83030c9ba6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "708d2a8c-b913-44f6-a0fa-f24d893a13a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Creation of dataset for training and validation\n",
    "x_train_np, x_val_np, y_train_np, y_val_np = train_test_split(\n",
    "    x_train_val_np, y_train_val_np, train_size=40_000, \n",
    "    stratify=y_train_val_np, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282dd87-695e-4cbc-b5de-bd9fdbc14bf2",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Write code to convert `x_train_np, y_train_np, x_val_np, y_val_np, x_test_np, y_test_np` to PyTorch Tensors.\n",
    "Name the tensors as `x_train, y_train, x_val, y_val, x_test, y_test`, respectively.\n",
    "\n",
    "**Note**: You may need to explicitly change the `dtype` of your tensors. PyTorch by default requires the type (`dtype`) of input tensor to be `torch.float32` and that of the labels to be `torch.int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f658ef6f-01dc-4f97-9149-498d34a33c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 2\n",
    "\n",
    "import torch\n",
    "\n",
    "#transformation of data to tensor for manipulation with PyTorch\n",
    "x_train = torch.from_numpy(x_train_np).to(dtype=torch.float32)\n",
    "y_train = torch.from_numpy(y_train_np).to(dtype=torch.int64)\n",
    "\n",
    "x_val = torch.from_numpy(x_val_np).to(dtype=torch.float32)\n",
    "y_val = torch.from_numpy(y_val_np).to(dtype=torch.int64)\n",
    "\n",
    "x_test = torch.from_numpy(x_test_np).to(dtype=torch.float32)\n",
    "y_test = torch.from_numpy(y_test_np).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29f160-2b66-4c6e-8443-b1c13067d6d9",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Write a Python class `CustomDataset` deriving `torch.utils.data.Dataset` and create dataloaders for the training, validation, and test sets.\n",
    "\n",
    "**Bonus** If possible, making the `transform` parameter and pass `ToTensor()` to it to avoid manually converting data to PyTorch tensors as in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "646ca6de-80eb-4090-847b-7476ca5cf369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 3\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#Class CustomDataset for facilitate data management and loading\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "        return data_point, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b16dfa2f-9760-4624-9cc7-7e87a7f075e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 3\n",
    "\n",
    "#Creation of dataloader for training, validation and test sets\n",
    "train_dataloader = DataLoader(CustomDataset(x_train, y_train), batch_size=64, shuffle=True)\n",
    "validation_dataloader = DataLoader(CustomDataset(x_val, y_val), batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(CustomDataset(x_test, y_test), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87425561-86ce-456e-a747-e508a0cb22b3",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Let the variable `device` be `'cuda'` if CUDA (GPU) is available. Otherwise, let it be `'cpu'`.\n",
    "(Do **not** move the tensors from Exercise 1 to this `device` yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fbca087-4535-4152-ae4d-3d394eed3c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa22fc-91f0-4616-950f-f1cabdaaf170",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Write a Python class `MLP5` for Multi-Layer Perceptron (MLP) with 5 layers derivng from `nn.Module` or `nn.Sequencial`.\n",
    "Your network must have the following sequential architecture:\n",
    "- First hidden layer: Linear layer (64 output features) + ReLU activation function\n",
    "- Second hidden layer: Linear layer (64 output features) + ReLU activation function\n",
    "- Third hidden layer: Linear layer (64 output features) + ReLU activation function\n",
    "- Forth hidden layer: Linear layer (64 output features) + ReLU activation function\n",
    "- Final layer: Linear layer\n",
    "\n",
    "Note that the final layer should have the output dimensionality equal to the number of classes in order to express class posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "240f28c3-c607-40de-8203-8ab9716bc840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 5\n",
    "\n",
    "# We first need to know the size of images\n",
    "image = x_train[0]\n",
    "dim = image.shape\n",
    "print(dim) # (color grey, dim_lenght, dim_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c033ba61-2d8b-4acc-8823-ef2498d8fa65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 5\n",
    "\n",
    "print(len(id2class)) #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4312c5d4-1f3f-4ef7-afe7-d00165e9a8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 5\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class MLP5(nn.Sequential):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Flatten() #Flatten the input\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(32*32, 64), #64 outputs\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(64, 64), #64 outputs\n",
    "            nn.ReLU() #activation function\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(64, 64), #64 outputs\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(64, 64), #64 outputs\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer5 = nn.Linear(64, 10) #10 outputs for the 10 classes\n",
    "    \n",
    "    def forward(self,x) : \n",
    "        # Define the forward pass through the network\n",
    "        y = self.layer0(x)\n",
    "        y = self.layer1(y)\n",
    "        y = self.layer2(y)\n",
    "        y = self.layer3(y)\n",
    "        y = self.layer4(y)\n",
    "        y = self.layer5(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bddaf1-3077-4f55-9d52-a019520fd30c",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "- Write a Python function for training a model with mini-batch updates for one epoch. Do not forget to move your mini-batch data to `device`.\n",
    "- Also, write Python function for evaluating the loss and the accuracy of a given model with a given dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09f8098b-f1a6-40a5-a00a-31243356050e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 6\n",
    "\n",
    "# Functions used for training and evaluation.\n",
    "# Taken from the previous practical session.\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Display loss from time to time\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss: >7f}  [{current: >5d} / {size: >5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"\\nTest set: \\n  Accuracy: {(100 * correct): >0.1f}%, Avg loss: {test_loss: >8f} \\n\")\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        test_loop(test_dataloader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0f65b-699f-40e4-9db0-e53668b549a8",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "- Create an object using your class and keep it in the `modelMLP5` variable. Do not forget move your model to `device`.\n",
    "- Choose any loss function.\n",
    "- Create an optimizer for optimizing `modelMLP5`.\n",
    "- Train `modelMLP5` with your function(s) for 10 epochs. During the training, print the training and validation loss/accuracy every epoch.\n",
    "\n",
    "You may need to tune hyper-parameters such as the learning rate later while observing the behavior of the model during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc1d5b4-68e0-458d-a936-8ba6b26da6ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 7\n",
    "\n",
    "# Creation of the MLP5 model object\n",
    "modelMLP5 = MLP5().to(device) # (TP2 - exo 4)\n",
    "loss_fn = nn.CrossEntropyLoss() #choose cross entropy loss\n",
    "optimizer = torch.optim.SGD(modelMLP5.parameters(), lr=1e-3) #choose learning rate at 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d544c404-a563-44b4-ad4e-2ddac4bdcde1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 7\n",
    "\n",
    "def do_epochs(model, loss_fn, optimizer, device, train_dataloader, test_dataloader, epochs) : \n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n---------------------\")\n",
    "         # Perform training for the current epoch using the training data : \n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "         # Evaluate the model's performance on the test data : \n",
    "        test_loop(test_dataloader, model,loss_fn,device)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4455fba-32ac-4142-8759-b85eec39a65f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 3.552095  [   64 / 40000]\n",
      "loss: 2.211643  [ 6464 / 40000]\n",
      "loss: 2.141806  [12864 / 40000]\n",
      "loss: 2.088259  [19264 / 40000]\n",
      "loss: 2.218909  [25664 / 40000]\n",
      "loss: 2.125087  [32064 / 40000]\n",
      "loss: 2.137391  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 26.1%, Avg loss: 2.038682 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.961048  [   64 / 40000]\n",
      "loss: 2.300394  [ 6464 / 40000]\n",
      "loss: 1.950344  [12864 / 40000]\n",
      "loss: 2.098875  [19264 / 40000]\n",
      "loss: 1.950861  [25664 / 40000]\n",
      "loss: 1.910197  [32064 / 40000]\n",
      "loss: 1.781519  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 28.5%, Avg loss: 1.971925 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.932552  [   64 / 40000]\n",
      "loss: 1.963396  [ 6464 / 40000]\n",
      "loss: 1.950960  [12864 / 40000]\n",
      "loss: 2.050962  [19264 / 40000]\n",
      "loss: 1.893093  [25664 / 40000]\n",
      "loss: 1.931257  [32064 / 40000]\n",
      "loss: 2.072082  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 29.4%, Avg loss: 1.940286 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.987325  [   64 / 40000]\n",
      "loss: 1.956935  [ 6464 / 40000]\n",
      "loss: 1.909952  [12864 / 40000]\n",
      "loss: 1.853707  [19264 / 40000]\n",
      "loss: 1.905280  [25664 / 40000]\n",
      "loss: 1.784217  [32064 / 40000]\n",
      "loss: 1.847355  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 30.6%, Avg loss: 1.930692 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.900168  [   64 / 40000]\n",
      "loss: 1.848224  [ 6464 / 40000]\n",
      "loss: 2.080647  [12864 / 40000]\n",
      "loss: 1.751097  [19264 / 40000]\n",
      "loss: 1.990120  [25664 / 40000]\n",
      "loss: 1.838801  [32064 / 40000]\n",
      "loss: 1.700088  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 29.4%, Avg loss: 1.958402 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 1.973169  [   64 / 40000]\n",
      "loss: 1.984158  [ 6464 / 40000]\n",
      "loss: 1.948626  [12864 / 40000]\n",
      "loss: 2.023828  [19264 / 40000]\n",
      "loss: 1.883448  [25664 / 40000]\n",
      "loss: 1.902901  [32064 / 40000]\n",
      "loss: 1.836446  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 31.9%, Avg loss: 1.872196 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 1.961670  [   64 / 40000]\n",
      "loss: 1.811262  [ 6464 / 40000]\n",
      "loss: 1.855011  [12864 / 40000]\n",
      "loss: 1.999507  [19264 / 40000]\n",
      "loss: 1.525676  [25664 / 40000]\n",
      "loss: 1.529666  [32064 / 40000]\n",
      "loss: 1.869943  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 33.6%, Avg loss: 1.855243 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 1.961583  [   64 / 40000]\n",
      "loss: 1.745746  [ 6464 / 40000]\n",
      "loss: 1.836608  [12864 / 40000]\n",
      "loss: 1.703541  [19264 / 40000]\n",
      "loss: 1.992985  [25664 / 40000]\n",
      "loss: 1.690702  [32064 / 40000]\n",
      "loss: 1.917446  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 33.3%, Avg loss: 1.856663 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 1.944049  [   64 / 40000]\n",
      "loss: 1.795131  [ 6464 / 40000]\n",
      "loss: 1.796659  [12864 / 40000]\n",
      "loss: 1.773378  [19264 / 40000]\n",
      "loss: 1.733273  [25664 / 40000]\n",
      "loss: 1.543377  [32064 / 40000]\n",
      "loss: 1.843815  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 34.7%, Avg loss: 1.831420 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 1.836679  [   64 / 40000]\n",
      "loss: 2.032496  [ 6464 / 40000]\n",
      "loss: 1.705845  [12864 / 40000]\n",
      "loss: 1.817195  [19264 / 40000]\n",
      "loss: 1.655080  [25664 / 40000]\n",
      "loss: 1.738780  [32064 / 40000]\n",
      "loss: 1.818494  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 35.0%, Avg loss: 1.833827 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 7\n",
    "\n",
    "# Training the MLP5 model for 10 epochs\n",
    "do_epochs(\n",
    "    model=modelMLP5,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01758752-10ad-41ff-a9ae-3670408e291c",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Write a Python class `MLP5BN` similarly to `MLP5`, but put a batch normalization layer (`torch.nn.BatchNorm1d`) before every activation layer.\n",
    "Then, create an object using your class and keep it in the `modelMLP5BN` variable. Train this model and compare the results for `modelMLP5` and `modelMLP5BN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69335fbf-f63f-478f-a39d-9ead47588ec1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 8\n",
    "\n",
    "class MLP5BN(nn.Sequential):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Flatten() #Flatten the input\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(32*32, 64), #64 outputs\n",
    "            nn.BatchNorm1d(64), #batch normalization layer\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64), #batch normalization layer\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64), #batch normalization layer\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64), #batch normalization layer\n",
    "            nn.ReLU() #ReLU activation function\n",
    "        )\n",
    "        self.layer5 = nn.Linear(64, 10) #10 outputs for the 10 classes\n",
    "    \n",
    "    def forward(self,x) : \n",
    "        # Define the forward pass through the network\n",
    "        y = self.layer0(x)\n",
    "        y = self.layer1(y)\n",
    "        y = self.layer2(y)\n",
    "        y = self.layer3(y)\n",
    "        y = self.layer4(y)\n",
    "        y = self.layer5(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "277ea53a-686c-452e-b4fa-d17ac03f3ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 8\n",
    "\n",
    "# Creation of the MLP5BN model object\n",
    "modelMLP5BN = MLP5BN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() #choose cross entropy loss\n",
    "optimizer = torch.optim.SGD(modelMLP5BN.parameters(), lr=1e-3) #choose learning rate at 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff7cb7cd-65b6-4274-8f56-8a87fe963eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.291671  [   64 / 40000]\n",
      "loss: 2.343551  [ 6464 / 40000]\n",
      "loss: 2.230405  [12864 / 40000]\n",
      "loss: 2.344657  [19264 / 40000]\n",
      "loss: 2.257828  [25664 / 40000]\n",
      "loss: 2.134750  [32064 / 40000]\n",
      "loss: 2.151238  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 23.0%, Avg loss: 2.160227 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 2.128495  [   64 / 40000]\n",
      "loss: 2.151046  [ 6464 / 40000]\n",
      "loss: 2.044288  [12864 / 40000]\n",
      "loss: 2.082887  [19264 / 40000]\n",
      "loss: 2.137089  [25664 / 40000]\n",
      "loss: 2.125490  [32064 / 40000]\n",
      "loss: 2.174444  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 27.5%, Avg loss: 2.069811 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 2.062126  [   64 / 40000]\n",
      "loss: 2.143394  [ 6464 / 40000]\n",
      "loss: 2.028516  [12864 / 40000]\n",
      "loss: 2.072208  [19264 / 40000]\n",
      "loss: 1.905382  [25664 / 40000]\n",
      "loss: 2.080150  [32064 / 40000]\n",
      "loss: 1.912730  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 30.1%, Avg loss: 2.007129 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 2.083869  [   64 / 40000]\n",
      "loss: 1.964683  [ 6464 / 40000]\n",
      "loss: 2.002419  [12864 / 40000]\n",
      "loss: 1.976416  [19264 / 40000]\n",
      "loss: 1.975780  [25664 / 40000]\n",
      "loss: 1.900621  [32064 / 40000]\n",
      "loss: 1.973567  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 31.3%, Avg loss: 1.963617 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.936124  [   64 / 40000]\n",
      "loss: 1.932051  [ 6464 / 40000]\n",
      "loss: 1.796042  [12864 / 40000]\n",
      "loss: 1.986172  [19264 / 40000]\n",
      "loss: 1.913895  [25664 / 40000]\n",
      "loss: 1.930206  [32064 / 40000]\n",
      "loss: 1.929110  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 32.4%, Avg loss: 1.925386 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 1.952107  [   64 / 40000]\n",
      "loss: 2.023334  [ 6464 / 40000]\n",
      "loss: 1.896773  [12864 / 40000]\n",
      "loss: 1.946694  [19264 / 40000]\n",
      "loss: 1.802388  [25664 / 40000]\n",
      "loss: 1.940018  [32064 / 40000]\n",
      "loss: 1.966499  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 33.1%, Avg loss: 1.904123 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 1.965831  [   64 / 40000]\n",
      "loss: 1.836412  [ 6464 / 40000]\n",
      "loss: 2.002910  [12864 / 40000]\n",
      "loss: 1.796482  [19264 / 40000]\n",
      "loss: 1.868690  [25664 / 40000]\n",
      "loss: 1.797510  [32064 / 40000]\n",
      "loss: 1.950808  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 34.0%, Avg loss: 1.876034 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 2.003615  [   64 / 40000]\n",
      "loss: 1.908702  [ 6464 / 40000]\n",
      "loss: 1.904942  [12864 / 40000]\n",
      "loss: 1.858053  [19264 / 40000]\n",
      "loss: 1.719688  [25664 / 40000]\n",
      "loss: 1.738929  [32064 / 40000]\n",
      "loss: 1.843520  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 34.6%, Avg loss: 1.855209 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 1.990676  [   64 / 40000]\n",
      "loss: 1.846123  [ 6464 / 40000]\n",
      "loss: 1.773256  [12864 / 40000]\n",
      "loss: 1.926475  [19264 / 40000]\n",
      "loss: 1.741870  [25664 / 40000]\n",
      "loss: 1.769371  [32064 / 40000]\n",
      "loss: 1.709746  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 35.2%, Avg loss: 1.840085 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 1.612115  [   64 / 40000]\n",
      "loss: 1.844955  [ 6464 / 40000]\n",
      "loss: 1.977055  [12864 / 40000]\n",
      "loss: 2.022661  [19264 / 40000]\n",
      "loss: 1.656446  [25664 / 40000]\n",
      "loss: 2.013592  [32064 / 40000]\n",
      "loss: 1.642293  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 35.6%, Avg loss: 1.828579 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 8\n",
    "\n",
    "# Training the MLP5BN model for 10 epochs\n",
    "do_epochs(\n",
    "    model=modelMLP5BN,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f53011a1-1779-472f-849c-e7c72f368f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution to Exercise 8\n",
    "\n",
    "# Creation of the MLP5BN model object WITH ANOTHER OPTIMIZER (ADAM)\n",
    "modelMLP5BN = MLP5BN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() #choose cross entropy loss\n",
    "optimizer = torch.optim.Adam(modelMLP5BN.parameters(), lr=1e-3) #choose learning rate at 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08f838f6-696f-48ad-8bfe-43aec1443a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.325582  [   64 / 40000]\n",
      "loss: 1.969161  [ 6464 / 40000]\n",
      "loss: 1.945705  [12864 / 40000]\n",
      "loss: 1.949350  [19264 / 40000]\n",
      "loss: 1.898749  [25664 / 40000]\n",
      "loss: 1.757084  [32064 / 40000]\n",
      "loss: 1.700575  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 34.9%, Avg loss: 1.838793 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.714568  [   64 / 40000]\n",
      "loss: 1.599748  [ 6464 / 40000]\n",
      "loss: 1.747428  [12864 / 40000]\n",
      "loss: 1.789474  [19264 / 40000]\n",
      "loss: 1.690956  [25664 / 40000]\n",
      "loss: 1.751548  [32064 / 40000]\n",
      "loss: 1.601255  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 37.6%, Avg loss: 1.739120 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.714399  [   64 / 40000]\n",
      "loss: 1.558341  [ 6464 / 40000]\n",
      "loss: 1.758484  [12864 / 40000]\n",
      "loss: 1.865996  [19264 / 40000]\n",
      "loss: 1.646223  [25664 / 40000]\n",
      "loss: 1.448921  [32064 / 40000]\n",
      "loss: 1.683453  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 39.5%, Avg loss: 1.721294 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.584676  [   64 / 40000]\n",
      "loss: 1.171224  [ 6464 / 40000]\n",
      "loss: 1.835417  [12864 / 40000]\n",
      "loss: 1.596685  [19264 / 40000]\n",
      "loss: 1.695690  [25664 / 40000]\n",
      "loss: 1.778110  [32064 / 40000]\n",
      "loss: 1.662276  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 35.1%, Avg loss: 1.829233 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.558370  [   64 / 40000]\n",
      "loss: 1.936430  [ 6464 / 40000]\n",
      "loss: 1.507710  [12864 / 40000]\n",
      "loss: 1.401508  [19264 / 40000]\n",
      "loss: 1.543962  [25664 / 40000]\n",
      "loss: 1.534725  [32064 / 40000]\n",
      "loss: 1.493899  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 40.4%, Avg loss: 1.692038 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 1.377255  [   64 / 40000]\n",
      "loss: 1.437732  [ 6464 / 40000]\n",
      "loss: 1.779421  [12864 / 40000]\n",
      "loss: 1.649977  [19264 / 40000]\n",
      "loss: 1.741837  [25664 / 40000]\n",
      "loss: 1.555662  [32064 / 40000]\n",
      "loss: 1.470279  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 40.6%, Avg loss: 1.677432 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 1.351302  [   64 / 40000]\n",
      "loss: 1.382432  [ 6464 / 40000]\n",
      "loss: 1.613522  [12864 / 40000]\n",
      "loss: 1.441158  [19264 / 40000]\n",
      "loss: 1.423113  [25664 / 40000]\n",
      "loss: 1.554263  [32064 / 40000]\n",
      "loss: 1.585167  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 42.3%, Avg loss: 1.641556 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 1.527098  [   64 / 40000]\n",
      "loss: 1.244859  [ 6464 / 40000]\n",
      "loss: 1.622925  [12864 / 40000]\n",
      "loss: 1.549026  [19264 / 40000]\n",
      "loss: 1.886117  [25664 / 40000]\n",
      "loss: 1.448148  [32064 / 40000]\n",
      "loss: 1.550627  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 41.1%, Avg loss: 1.664521 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 1.589910  [   64 / 40000]\n",
      "loss: 1.387113  [ 6464 / 40000]\n",
      "loss: 1.513353  [12864 / 40000]\n",
      "loss: 1.399799  [19264 / 40000]\n",
      "loss: 1.520679  [25664 / 40000]\n",
      "loss: 1.403550  [32064 / 40000]\n",
      "loss: 1.567525  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 40.6%, Avg loss: 1.678093 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 1.377186  [   64 / 40000]\n",
      "loss: 1.308804  [ 6464 / 40000]\n",
      "loss: 1.252645  [12864 / 40000]\n",
      "loss: 1.450530  [19264 / 40000]\n",
      "loss: 1.469082  [25664 / 40000]\n",
      "loss: 1.389300  [32064 / 40000]\n",
      "loss: 1.443511  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 41.4%, Avg loss: 1.666859 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 8\n",
    "\n",
    "# Training the MLP5BN model for 10 epochs\n",
    "do_epochs(\n",
    "    model=modelMLP5BN,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a160a6d-a455-4155-ad5b-fa5e8aea4fdf",
   "metadata": {},
   "source": [
    "### Solution to Exercise 8\n",
    "\n",
    "**Optimizer change** : \n",
    "We have also tried changing the optimizer in this question, as using Adam gives better results in terms of accuracy. Indeed, with the MLP5BN model, the SGD optimizer gives an accuracy of 35.6%, whereas the Adam optimizer gives a result of 41.2%. The difference is even greater in subsequent models. For this reason, we will use the Adam optimizer for the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84b468-53c6-4807-b45f-3ad408e0ad66",
   "metadata": {
    "tags": []
   },
   "source": [
    "The MLP5BN model performs better than the MLP5 model because of accuracy.\n",
    "- MLP5 accuracy is 34.2% at the last epoch. \n",
    "- MLP5BN accuracy is 42% at the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b9bf7-aef8-41d2-badd-98f4f55f6f9c",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "The following Python class `LeNet5` is an implementation of a Convolutional Neural Network (CNN).\n",
    "Train this model and compare it with the previous two models. (Ignore the `num_channels` parameter of the class for now. Set it to the default value `1`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd362515-d777-450b-91f2-9bdb8933dfaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)  # Second dimension is for channels, but we only have one channel.\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "522d6194-11e9-43a1-b350-348dd30c1f59",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 9\n",
    "\n",
    "# Creation of the LeNet5 model object\n",
    "modelLeNet5 = LeNet5().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() #choose cross entropy loss\n",
    "optimizer = torch.optim.Adam(modelLeNet5.parameters(), lr=1e-3) #choose learning rate at 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "884dcea3-bde2-4648-b63a-bf8d5dc5e07f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.309488  [   64 / 40000]\n",
      "loss: 1.714836  [ 6464 / 40000]\n",
      "loss: 1.581706  [12864 / 40000]\n",
      "loss: 1.436639  [19264 / 40000]\n",
      "loss: 1.518769  [25664 / 40000]\n",
      "loss: 1.582320  [32064 / 40000]\n",
      "loss: 1.545920  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 48.4%, Avg loss: 1.458222 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.497009  [   64 / 40000]\n",
      "loss: 1.166445  [ 6464 / 40000]\n",
      "loss: 1.523243  [12864 / 40000]\n",
      "loss: 1.161925  [19264 / 40000]\n",
      "loss: 1.309555  [25664 / 40000]\n",
      "loss: 1.064916  [32064 / 40000]\n",
      "loss: 1.231855  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 46.4%, Avg loss: 1.507775 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.346433  [   64 / 40000]\n",
      "loss: 1.178074  [ 6464 / 40000]\n",
      "loss: 1.281491  [12864 / 40000]\n",
      "loss: 1.136083  [19264 / 40000]\n",
      "loss: 1.309618  [25664 / 40000]\n",
      "loss: 1.136078  [32064 / 40000]\n",
      "loss: 1.019476  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 54.9%, Avg loss: 1.260650 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 0.813928  [   64 / 40000]\n",
      "loss: 1.150140  [ 6464 / 40000]\n",
      "loss: 1.201090  [12864 / 40000]\n",
      "loss: 1.049262  [19264 / 40000]\n",
      "loss: 1.165854  [25664 / 40000]\n",
      "loss: 1.181906  [32064 / 40000]\n",
      "loss: 0.887393  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 57.1%, Avg loss: 1.203399 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.089473  [   64 / 40000]\n",
      "loss: 1.161806  [ 6464 / 40000]\n",
      "loss: 1.224532  [12864 / 40000]\n",
      "loss: 0.940909  [19264 / 40000]\n",
      "loss: 0.880924  [25664 / 40000]\n",
      "loss: 1.308024  [32064 / 40000]\n",
      "loss: 1.067526  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 52.7%, Avg loss: 1.391091 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 1.240304  [   64 / 40000]\n",
      "loss: 0.869189  [ 6464 / 40000]\n",
      "loss: 1.052693  [12864 / 40000]\n",
      "loss: 1.212447  [19264 / 40000]\n",
      "loss: 1.347697  [25664 / 40000]\n",
      "loss: 1.031390  [32064 / 40000]\n",
      "loss: 0.643726  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 60.0%, Avg loss: 1.150622 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 0.976704  [   64 / 40000]\n",
      "loss: 1.144038  [ 6464 / 40000]\n",
      "loss: 1.150168  [12864 / 40000]\n",
      "loss: 1.243149  [19264 / 40000]\n",
      "loss: 1.064017  [25664 / 40000]\n",
      "loss: 0.818913  [32064 / 40000]\n",
      "loss: 0.805473  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 58.1%, Avg loss: 1.260688 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 0.665938  [   64 / 40000]\n",
      "loss: 0.958952  [ 6464 / 40000]\n",
      "loss: 0.952239  [12864 / 40000]\n",
      "loss: 0.933454  [19264 / 40000]\n",
      "loss: 0.836940  [25664 / 40000]\n",
      "loss: 0.916155  [32064 / 40000]\n",
      "loss: 0.953792  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 45.4%, Avg loss: 1.852420 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 0.929577  [   64 / 40000]\n",
      "loss: 0.941719  [ 6464 / 40000]\n",
      "loss: 0.763433  [12864 / 40000]\n",
      "loss: 0.650891  [19264 / 40000]\n",
      "loss: 0.824877  [25664 / 40000]\n",
      "loss: 0.596407  [32064 / 40000]\n",
      "loss: 1.093455  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 60.8%, Avg loss: 1.142737 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 1.034481  [   64 / 40000]\n",
      "loss: 0.962264  [ 6464 / 40000]\n",
      "loss: 0.799211  [12864 / 40000]\n",
      "loss: 0.776036  [19264 / 40000]\n",
      "loss: 0.826796  [25664 / 40000]\n",
      "loss: 0.893002  [32064 / 40000]\n",
      "loss: 0.919568  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 61.4%, Avg loss: 1.129360 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 9\n",
    "\n",
    "# Training the LeNet5 model for 10 epochs\n",
    "do_epochs(\n",
    "    model=modelLeNet5,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636f230-9dc4-4c94-a674-3dd6b138261e",
   "metadata": {},
   "source": [
    "The LeNet5 model performs better than the MLP5BN and MLP5 model because of accuracy.\n",
    "- MLP5 accuracy is 34.2% at the last epoch. \n",
    "- MLP5BN accuracy is 42.0% at the last epoch.\n",
    "- LeNet5 accuracy is 62.1% at the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5f75c-79c9-4058-995c-c73d176ab7f8",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "Below is the same dataset but with colors. For expressing the intensities for the red, green, blue colors, each image has 3 *channels* now, which is why the shape of each image is `(3, 32, 32)`.\n",
    "The goal of this exercise is to train a classifier using `LeNet5`, but the `LeNet5` class defined above assumes that input images have only one channel, so we need to rewrite the class a bit. For that, read [the documentation for `torch.nn.Conv2d` class](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) to understand how to modify the first layer of the `LeNet5` class. Write code for this modified class, naming it `LeNet5Color`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71796fb0-1f80-4a5c-9753-4648c48ad3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truck', 'truck', 'deer', 'automobile', 'automobile']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an image: (3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "x_train_val_np_color, y_train_val_np_color = load_cifar10(train=True, color=True)\n",
    "x_test_np_color, y_test_np_color = load_cifar10(train=False, color=True)\n",
    "plot_images(x_train_val_np_color, y_train_val_np_color, rows=1, cols=5, color=True)\n",
    "print(f'Shape of an image: {x_train_val_np_color[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c1aa3d8-0d79-4747-828c-7ac91393712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution to Exercise 10\n",
    "\n",
    "class LeNet5Color(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5Color, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),  # MODIFICATION HERE ONLY : Input has 3 channels\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0acd0-b720-41e2-a6e4-0e7ac13070a1",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "Split the data, create dataloaders, train an instance of `LeNet5Color`, and compare it with `LeNet5` using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70673a87-abb2-4414-9c08-7aaf1c4a49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution to Exercise\n",
    "\n",
    "# Split the data\n",
    "x_train_np_color, x_val_np_color, y_train_np_color, y_val_np_color = train_test_split(\n",
    "    x_train_val_np_color, y_train_val_np_color, train_size=40_000, \n",
    "    stratify=y_train_val_np_color, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf8b153d-22d9-48fb-8e0a-62e0a58a4fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 11\n",
    "\n",
    "# Transformation of data to tensor for manipulation with PyTorch\n",
    "x_train_color = torch.from_numpy(x_train_np_color).to(dtype=torch.float32)\n",
    "y_train_color = torch.from_numpy(y_train_np_color).to(dtype=torch.int64)\n",
    "x_val_color = torch.from_numpy(x_val_np_color).to(dtype=torch.float32)\n",
    "y_val_color = torch.from_numpy(y_val_np_color).to(dtype=torch.int64)\n",
    "x_test_color = torch.from_numpy(x_test_np_color).to(dtype=torch.float32)\n",
    "y_test_color = torch.from_numpy(y_test_np_color).to(dtype=torch.int64)\n",
    "\n",
    "#Creation of dataloader for training, validation and test sets\n",
    "train_dataloader2 = DataLoader(CustomDataset(x_train_color, y_train_color), batch_size=64, shuffle=True)\n",
    "validation_dataloader2 = DataLoader(CustomDataset(x_val_color, y_val_color), batch_size=64, shuffle=True)\n",
    "test_dataloader2 = DataLoader(CustomDataset(x_test_color, y_test_color), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6c867ec-5f08-4b65-baa1-985320887291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Solution to Exercise 11\n",
    "\n",
    "#Creation of the LeNet5 model object\n",
    "modelLeNet5Color = LeNet5Color().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() #choose cross entropy loss\n",
    "optimizer = torch.optim.Adam(modelLeNet5Color.parameters(), lr=1e-3) #choose learning rate at 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d4076bf-4b4e-4d25-839d-69faa584e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.313484  [   64 / 40000]\n",
      "loss: 1.624957  [ 6464 / 40000]\n",
      "loss: 1.445046  [12864 / 40000]\n",
      "loss: 1.546689  [19264 / 40000]\n",
      "loss: 1.483902  [25664 / 40000]\n",
      "loss: 1.253821  [32064 / 40000]\n",
      "loss: 1.302045  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 49.5%, Avg loss: 1.387416 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.296285  [   64 / 40000]\n",
      "loss: 1.523398  [ 6464 / 40000]\n",
      "loss: 1.289726  [12864 / 40000]\n",
      "loss: 1.324071  [19264 / 40000]\n",
      "loss: 1.223495  [25664 / 40000]\n",
      "loss: 1.400558  [32064 / 40000]\n",
      "loss: 1.456233  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 54.4%, Avg loss: 1.296428 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.267114  [   64 / 40000]\n",
      "loss: 1.119079  [ 6464 / 40000]\n",
      "loss: 1.250340  [12864 / 40000]\n",
      "loss: 1.222034  [19264 / 40000]\n",
      "loss: 0.951620  [25664 / 40000]\n",
      "loss: 1.145148  [32064 / 40000]\n",
      "loss: 1.212294  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 58.2%, Avg loss: 1.169376 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.106560  [   64 / 40000]\n",
      "loss: 1.092519  [ 6464 / 40000]\n",
      "loss: 1.368763  [12864 / 40000]\n",
      "loss: 1.079606  [19264 / 40000]\n",
      "loss: 1.197501  [25664 / 40000]\n",
      "loss: 0.992340  [32064 / 40000]\n",
      "loss: 0.844136  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 49.0%, Avg loss: 1.531261 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.174051  [   64 / 40000]\n",
      "loss: 1.171356  [ 6464 / 40000]\n",
      "loss: 0.992759  [12864 / 40000]\n",
      "loss: 0.877433  [19264 / 40000]\n",
      "loss: 1.157121  [25664 / 40000]\n",
      "loss: 1.022887  [32064 / 40000]\n",
      "loss: 1.192503  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 51.4%, Avg loss: 1.390822 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 1.123038  [   64 / 40000]\n",
      "loss: 1.001166  [ 6464 / 40000]\n",
      "loss: 1.153094  [12864 / 40000]\n",
      "loss: 0.984779  [19264 / 40000]\n",
      "loss: 0.888737  [25664 / 40000]\n",
      "loss: 1.126747  [32064 / 40000]\n",
      "loss: 1.068766  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 59.2%, Avg loss: 1.191159 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 0.922167  [   64 / 40000]\n",
      "loss: 0.921680  [ 6464 / 40000]\n",
      "loss: 0.779330  [12864 / 40000]\n",
      "loss: 0.968790  [19264 / 40000]\n",
      "loss: 0.832845  [25664 / 40000]\n",
      "loss: 1.145478  [32064 / 40000]\n",
      "loss: 0.844414  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 62.6%, Avg loss: 1.083599 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 0.891066  [   64 / 40000]\n",
      "loss: 0.903866  [ 6464 / 40000]\n",
      "loss: 0.996702  [12864 / 40000]\n",
      "loss: 0.811449  [19264 / 40000]\n",
      "loss: 1.120463  [25664 / 40000]\n",
      "loss: 0.692332  [32064 / 40000]\n",
      "loss: 1.047023  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 63.8%, Avg loss: 1.036446 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 0.867489  [   64 / 40000]\n",
      "loss: 0.765603  [ 6464 / 40000]\n",
      "loss: 0.945012  [12864 / 40000]\n",
      "loss: 0.939018  [19264 / 40000]\n",
      "loss: 0.742140  [25664 / 40000]\n",
      "loss: 0.780992  [32064 / 40000]\n",
      "loss: 0.703696  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 61.3%, Avg loss: 1.120822 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 0.746884  [   64 / 40000]\n",
      "loss: 0.693242  [ 6464 / 40000]\n",
      "loss: 0.811613  [12864 / 40000]\n",
      "loss: 1.170264  [19264 / 40000]\n",
      "loss: 0.878447  [25664 / 40000]\n",
      "loss: 0.889416  [32064 / 40000]\n",
      "loss: 1.009630  [38464 / 40000]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 62.7%, Avg loss: 1.068085 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 11\n",
    "\n",
    "# Training the LeNet5 model for 5 epochs\n",
    "do_epochs(\n",
    "    model=modelLeNet5Color,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader2,\n",
    "    test_dataloader=test_dataloader2,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858773e-7d9d-4cf5-b118-b93a6ee5eea6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The LeNet5Color model performs better than the LeNet5, MLP5BN and MLP5 model because of accuracy.\n",
    "- MLP5 accuracy is 34.2% at the last epoch. \n",
    "- MLP5BN accuracy is 42.0% at the last epoch.\n",
    "- LeNet5 accuracy is 62.1% at the last epoch.\n",
    "- LeNet5Color accuracy is 63.4% at the last epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa2d43-fea2-4615-940f-a831e1438bfc",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "- Randomly take 10 examples from each class of CIFAR10 to make a subset (*) of the dataset.\n",
    "- Load this model ResNet18 pre-trained on ImageNet v1 which is available from `torchvision.models`. See [this](https://pytorch.org/vision/stable/models.html) and [this documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18).\n",
    "- Fine-tune the pre-trained model for 10-class classification with your subset (from (*)) of CIFAR10 (with colors). Train the model for 5 epochs.\n",
    "- You may need to carefully read [this documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18) to do the right data pre-processing.\n",
    "\n",
    "For this exercise, what is important is to have correct code. The final accuracy is less important, so you don't need to spend too much time on tuning hyper-parameters. There is a big difference in the image sizes, and it is challenging to make this transfer learning successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "86d42a85-b818-4cfe-93aa-bfcde12005ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution to Exercise 12\n",
    "\n",
    "### Creation of the subset dataset\n",
    "\n",
    "# Definition of a function which create subsets with 10 examples from each 10 classes\n",
    "\n",
    "def create_subset(data, labels, num_per_class=10):\n",
    "    unique_labels = np.unique(labels)\n",
    "    subset_data = []\n",
    "    subset_labels = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        np.random.shuffle(indices)\n",
    "        selected_indices = indices[:num_per_class]\n",
    "        \n",
    "        subset_data.append(data[selected_indices])\n",
    "        subset_labels.extend([label] * num_per_class)\n",
    "    \n",
    "    subset_data = np.concatenate(subset_data, axis=0)\n",
    "    subset_labels = np.array(subset_labels)\n",
    "    \n",
    "    return subset_data, subset_labels\n",
    "\n",
    "\n",
    "# Create subsets for the training, validation, and test datasets\n",
    "\n",
    "x_train_col_subset, y_train_col_subset = create_subset(x_train_color, y_train_color)\n",
    "x_val_col_subset, y_val_col_subset = create_subset(x_val_color, y_val_color)\n",
    "x_test_col_subset, y_test_col_subset = create_subset(x_test_color, y_test_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c4dacd4-8149-46aa-8582-eaaf3a7e94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution to Exercise 12\n",
    "\n",
    "# Convert the subsets to PyTorch tensors\n",
    "\n",
    "x_train_col_subset = torch.from_numpy(x_train_col_subset).to(dtype=torch.float32)\n",
    "y_train_col_subset = torch.from_numpy(y_train_col_subset).to(dtype=torch.int64)\n",
    "x_val_col_subset = torch.from_numpy(x_val_col_subset).to(dtype=torch.float32)\n",
    "y_val_col_subset = torch.from_numpy(y_val_col_subset).to(dtype=torch.int64)\n",
    "x_test_col_subset = torch.from_numpy(x_test_col_subset).to(dtype=torch.float32)\n",
    "y_test_col_subset = torch.from_numpy(y_test_col_subset).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "98f0a6f5-ada7-441d-8418-3c7da7870a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution to Exercise 12\n",
    "\n",
    "# Create Dataloaders for the subsets\n",
    "\n",
    "train_dataloader_col_subset = DataLoader(CustomDataset(x_train_col_subset, y_train_col_subset), batch_size=64, shuffle=True)\n",
    "validation_dataloader_col_subset = DataLoader(CustomDataset(x_val_col_subset, y_val_col_subset), batch_size=64, shuffle=True)\n",
    "test_dataloader_col_subset = DataLoader(CustomDataset(x_test_col_subset, y_test_col_subset), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eee0828b-9a20-4971-9c37-176fa0ed88aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 12\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet-18 model\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer for 10 classes\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # choose cross entropy loss function\n",
    "optimizerSGD = torch.optim.SGD(resnet18.parameters(), lr=1e-3, momentum=0.9)\n",
    "optimizerAdam = torch.optim.Adam(resnet18.parameters(), lr=1e-3)\n",
    "\n",
    "train_dataset = CustomDataset(x_train_col_subset, y_train_col_subset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b5904f02-f6ea-4cf1-a81d-217b20c44660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.641489  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 9.0%, Avg loss: 2.518929 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 2.351585  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 12.0%, Avg loss: 2.436461 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.840243  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 6.0%, Avg loss: 2.383144 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.385954  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 7.0%, Avg loss: 2.311705 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.108216  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 11.0%, Avg loss: 2.269379 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 12\n",
    "\n",
    "# Training the LeNet5 model for 5 epochs with SGD optimizer\n",
    "do_epochs(\n",
    "    model=resnet18,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizerSGD,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader_col_subset,\n",
    "    test_dataloader=test_dataloader_col_subset,\n",
    "    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e2bf4e63-7d8a-48b5-9a77-8a8fc53ea8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 0.846418  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 29.0%, Avg loss: 2.126877 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 0.834293  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 27.0%, Avg loss: 2.091522 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 0.208018  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 25.0%, Avg loss: 2.121884 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 0.070009  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 26.0%, Avg loss: 2.109535 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 0.013909  [   64 /   100]\n",
      "\n",
      "Test set: \n",
      "  Accuracy: 25.0%, Avg loss: 2.094159 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Solution to Exercise 12\n",
    "\n",
    "# Training the LeNet5 model for 5 epochs with Adam optimizer\n",
    "do_epochs(\n",
    "    model=resnet18,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizerAdam,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader_col_subset,\n",
    "    test_dataloader=test_dataloader_col_subset,\n",
    "    epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
